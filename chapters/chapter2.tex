%!TEX program=luatex

\newpage

\section{Introduction}
Avec la montée du Web 2.0, la production de contenu participatif et collaboratif a largement remplacé les méthodes traditionnelles de partage de l'information. Un vaste potentiel inexploité réside dans les quantités de données à la fois énorme et dynamique disponibles sur les différentes plate-formes numériques qui ne peuvent être traités avec les méthodes de l'informatique classique.\\
Dans ce chapitre, nous allons discuter du Traitement Automatique de la Langage Naturel (TALN), de sa puissance et son importance.\\ 
En bref, l'objectif de cette parti est de décrire le but, les procédures et les applications pratiques du TALN d'une manière simple et claire. Nous examinerons la littérature la plus récente tout en décrivant les méthodes et les processus disponibles, nous analyserons certains des défis auxquels les chercheurs sont confrontés, et passerons brièvement en revue certaines des applications actuelles et futures de cette science.

\section{Définition}
Le Traitement automatique du langage naturel (TALN) est l'un des principaux domaines de l'intelligence artificielle, il permet de développer des systèmes qui peuvent \emph{analyser} et \emph{comprendre} le langage humain. En dehors des opérations courantes de traitement de texte qui considèrent les textes comme une simple séquence de symboles, l'utilisation des techniques de TALN permet d'organiser et de structurer des quantités énormes de connaissances pour développer des applications très avancées telles que la synthèse automatique, la reconnaissance d'entités, l'analyse des sentiments, la reconnaissance de la parole, etc.\\
Le TALN est considéré comme un problème difficile en informatique dû à l'imprécision du langage humain. Comprendre le langage humain, c'est comprendre non seulement les mots, mais aussi les concepts et comment ils sont liés pour créer du sens. Bien que la langue soit l'une des tâches les plus faciles à apprendre pour les humains, l'ambiguïté rend le traitement de cette dernière difficile à maîtriser pour les ordinateurs.

\section{Domaines d'applications}
Le TALN est utilisée pour manipuler le langage humain, qu'il s'agisse d'extraire du sens ou de générer du texte dans le but d'accomplir des tâches tels que le résumé automatique d'un document, la traduction entre deux langages naturels ou la détection des spams.

On peut distinguer deux grands domaines où les techniques du traitement automatique de la langue ont pu faire des avancés énormes, la \emph{recherche scientifique} et \emph{l'industrie informatique}.\\
%Recherche scientifique
Dans les laboratoires de recherches en intelligence artificielle, le TALN est considéré, souvent, comme l'une des branches les plus importantes et les plus productives.\\ 
De nombreuses activités cognitives qui se produisent dans l'esprit humain ont pu être simuler grâce à ses techniques.
On peut cité: 
\begin{itemize}
    \item \textbf{La traduction automatique:}
    La traduction automatique des textes est probablement l'un des domaines les plus connu de l'IA, Elle a fait l'objet de plusieurs travaux depuis très longtemps. Le processus de traduction est découpé en plusieurs phases successives. Tout d'abord la compréhension et l'assimilation, la déverbalisation et la conservation du sens, ensuite la réexpression et la reformulation en langue cible.\\
    Le traducteur automatique le plus utilisé sur internet est \emph{Google Translate} développé par le département de traduction automatique de \emph{Google} en 2006, il supporte maintenant plus de 103 langues.

    \item \textbf{Le résumé automatique:}
    La construction automatique de résumés est un champ de recherche originale au sein de l'informatique, même si son ampleur n'a jamais été aussi importante que la traduction automatique.\\
    Plusieurs approches ont été proposées, premièrement des systèmes qui permettent l'élaboration automatique de résumés à partir de l'extraction de phrases. Ensuite, et avec le développement des outils informatiques (logiciels et matériels), la construction de résumés s'est basé sur le fait de donner au programme informatique la capacité d'élaborer des abstractions à partir de la \emph{compréhension} des textes.

    \item \textbf{La classification de texte/document:}
    La classification automatique de documents/textes est un problème connu en informatique, il s'agit d'assigner un document/texte à une ou plusieurs catégories ou classes. Le problème est différent selon la nature des documents/textes en question.\\
    L'idée générale consiste en l'identification et l'extraction des éléments pertinents à partir d'un texte/document contenant des informations dont la nature est spécifiée à l'avance. Elle vise donc à transformer un texte de son format initial (une suite de chaînes de caractères) à une représentation structurée et donc un format compréhensible par l'ordinateur.\\
\end{itemize}

%Industrie
Dans l'industrie, et avec le coût du calcul qui ne cesse de baisser, l'évolution exponentielles des algorithmes et surtout la disponibilité des données sur les différents supports numériques, les entreprises ont commencé à s'intéresser à l'analyse et l'exploitation de ces quantités massives de connaissances.\\
Grâce au TALN on a pu trouver des réponses aux différentes questions fréquentes. 
\begin{itemize}
    \item \textbf{Service Client:}\\
    Fortement utilisées dans le service client, les techniques de TALN permettent de développer des systèmes capables de simuler les interactions entre les clients et les entreprises. Des systèmes qui pointent vers les raisons de l'insatisfaction/satisfaction des consommateurs.\\
    De nombreuses entreprises analysent maintenant les enregistrements d'appels clients, les conversations sur les réseaux sociaux et les commentaires sur les forums. Ils déploient également des robots de discussion et des assistants en ligne automatisés pour fournir une réponse immédiate aux besoins simples et réduire la charge de travail pour leurs employés. On peut citer: 
    \begin{itemize}
        \item \textbf{La reconnaissance vocale:} convertit le langage parlé en texte. Les progrès de l'apprentissage profond (Deep Learning) au cours des dernières années et les quantités massives de données disponibles sur internet ont permis de déployer cette technologie dans des systèmes commerciaux tels que Siri d'Apple, Alexa d'Amazon et Google Assistant/Home dernièrement.
        \item \textbf{Système de Question/Réponses:} répondre aux questions posées par les humains dans une langue naturelle. La technologie est utilisée aujourd'hui par de nombreuses entreprises pour les chatbots, à la fois pour les projets internes (RH, opérations) et externes (service client). Ces systèmes sont implémentés, pratiquement en natif, sur tout les systèmes d'exploitations mobile (Android, IOS).\\
    \end{itemize}

    \item \textbf{E-réputation:}\\
    Les entreprises ont commencé, et cela depuis les années 80, à utiliser des logiciels pour trouver des modèles dans leurs propres données et prendre de meilleures décisions. L'optimisation des chaînes d'approvisionnement, des inventaires et des entrepôts, des processus de vente et de nombreuses autres applications ont donné naissance à ce que nous appelons aujourd'hui le \emph{Business Intelligence}\footnote{Stratégies et technologies utilisées par les entreprises pour l'analyse des données} (BI).\\ 
    Mais pour une entreprise le plus important et le plus précieux est ce qui est dit dehors. C'est ce qui a poussé ces derniers à adopter des outils qui permettent d'exploiter les données externes/publiques collectées sur les réseaux sociaux.\\
    Certaines de ces données sont structurées et prêtes à être analysées, par contre la plus grande partie générés par l'homme tels que les articles de blog, commentaires sur les forums ou les offres d'emploi reste non structurée. Ces sources contiennent des informations précieuses sur l'évolution des concurrents, des clients et du marché dans son ensemble.\\
    Et comme les consommateurs formulent leurs plaintes de plus en plus sur \emph{Facebook} et \emph{Twitter}, la surveillance et la gestion de la e-réputation sont devenues une priorité pour les entreprises.
    \begin{itemize}
        \item \textbf{L'analyse de sentiment:} déterminer l'attitude, l'état émotionnel, le jugement ou l'intention de l'internaute (positive, neutre ou négative) ou aussi reconnaître l'humeur (heureux, triste, calme, en colère ...).\\
    \end{itemize}

    \item \textbf{Publicité:}\\
    Les emails, les médias sociaux, le commerce électronique et les comportements sur les navigateurs contiennent beaucoup d'informations sur ce qui nous intéresse vraiment. L'énorme potentiel de ce type de données non structurées est confirmé par le fait que les plus grandes entreprises génèrent aujourd'hui le plus de de leurs recettes de vente d'annonces (Google et Facebook).\\ Les tâches TALN comprennent:
    \begin{itemize}
        \item \textbf{Correspondance par mot-clé (matching):} vérifie si des mots d'intérêt sont inclus dans un texte. 
        \item \textbf{Désambiguïsation:} identification du sens d'un mot utilisé dans une phrase.\\\\
    \end{itemize}
\end{itemize}

    
\section{Outils de base}
Afin de réaliser une des applications sus-citées, le processus comporte plusieurs phases, commençant par la collecte de données, le pré-traitement et la construction des corpus (Datasets), toute ces étapes nécessitent de l'expertise et la maîtrise des techniques de TALN, et c'est à ce moment là où on peut exploiter ces connaissances structurées.

Parmi les techniques du traitement automatique de la langue les plus importantes, on trouve:
    \subsection{Reconnaissance de patrons}
    Un patron servira à retrouver dans un texte des formes ayant une même construction. Souvent, le \emph{pattenr matching} se fait en utilisant les expressions régulières.\\ 
    Une expression régulière est une chaîne de caractères qui décrit, selon une syntaxe précise, un ensemble de chaînes de caractères possibles. Elles sont utilisées pour programmer des logiciels avec des fonctionnalités de lecture, de contrôle, de modification, et d'analyse de textes.\\
    On peut les retrouver dans plusieurs utilitaires tel que \textbf{GNU grep}, implémenté dans le noyaux \emph{Linux}, qui utilisent ces expressions pour parcourir de façon automatique un document à la recherche de morceaux de texte compatibles avec le motif de défini, et éventuellement effectuer un ajout, une substitution ou une suppression.
    \begin{lstlisting}[style=code]
        #Permet la reconnaissance des adresses mails
        [\w+.-]+@[\w.-]+\.[a-zA-Z]{2,}
    \end{lstlisting}

    \subsection{Segmentation (Tokenization)}
    C'est l'opération la plus basique dans un processus de TALN. Elle consiste en l'identification des Tokens\footnote{désigne une entité (ou unité) lexicale}, ou de phrases entières dans un texte que nous voulons traiter.\\La difficulté est dans le fait que l'utilisation de la ponctuation et les séparateurs pour la segmentation, et dans plusieurs langues dont l'anglais et l'arabe, est souvent ambigu.     
    De nombreux algorithmes de segmentation appelés \emph{Tokenizer} sont disponibles sur internet en libre accès:
    % \begin{itemize}
    %     \item \textbf{RegexpTokenizer:} divise une chaîne en sous-chaînes en utilisant une expression régulière.
    %     \item \textbf{TweetTokenizer:} développé en 2016 afin de pouvoir segmenter des Tweets en tokens, dans le but d'exploiter le contenu énorme des données disponible sur Twitter.
    %     \item \textbf{PTBTokenizer:} c'est un programme open source\footnote{un programme informatique dont le code source est distribué sous une licence permettant à quiconque de le lire, le modifier ou le redistribuer.}, l'implémentation est basé sur des règles.  
    % \end{itemize}

    \subsection{Lemmatisation et racinisation}
    La racine d'un mot correspond à la partie du mot restante une fois que l'on a supprimé son préfixe et son suffixe (et infixes dans certaines langue comme l'Arabe), à savoir son radical. Elle est aussi parfois connu sous le nom de \emph{Stemme} d'un mot.\\ 
    Contrairement au \emph{Lemme} qui correspond à un mot réel de la langue, la racine (Stemme) ne correspond généralement pas à un mot réel.
    \begin{lstlisting}[style=code]
        #Lemmatisation 
            "chercher" => "cherch"
        #Racinisation (Stemming):
            "frontalier" => "front"  
    \end{lstlisting}
    Plusieurs outils destinés à la lemmatisation et la racinisation sont implémentés dans des librairies majoritairement open source et dans différents langages de programmations.
    % \begin{itemize}
    %     \item \textbf{Porter Stemmer:} développé en 1979 par \emph{Martin Porter}  à Cambridge (Angleterre). 
    %     L'algorithme permet d'éliminer les terminaisons morphologiques des mots en Anglais.
    %     \item \textbf{Snowball stemmer:} mis en place par un groupe de linguiste, il prend en charge officiellement 14 langues dont l'Anglais et le Français. 
    %     \item \textbf{Tashaphyne:} écrit entièrement en Python, \emph{Tashaphyne} est développé en 2012 par l'algérien Taha Zerrouki. Il est destiné au traitement de la langue Arabe uniquement.\cite{zerrouki2012tashaphyne}
    % \end{itemize} 

    \subsection{L'étiquetage morpho-syntaxique (PoS Tagging)}
    Le PoS Tagging est le processus qui consiste à associer à chaque mot d'un texte les informations grammaticales correspondantes comme le genre, le nombre, etc. avec l'utilisation des programmes informatiques.\\
    L'étiquetage morpho-syntaxique est une opération très complexe, le fait d'avoir des mots et leur étiquettes est souvent insuffisant vu les ambiguïtés qu'on peut rencontrer (pour un même mot, différentes étiquettes possible).
    \begin{lstlisting}[style=code]
        """Phrase"""        Le  paysan ferme la  ferme
        """Étiquetage 1"""  DET NN     V     DET NN
        """Étiquetage 2"""  DET NN     ADJ   PRN V
    \end{lstlisting}
    Pour la langue anglaise on peut distinguer entre 50 et 150 étiquettes morpho-syntaxique selon le besoins et la précision voulues.
    % Il existe également, comme pour les Stemmer, un grand ensemble d'algorithmes de PoS Tagging (pré)entrainé:
    % \begin{itemize}
    %     \item \textbf{Stanford PoS Tagger:} Écrit en Java dans ça totalité, le Stanford PoS Tagger reste l'un des meilleurs algorithmes d'étiquetage morpho-syntaxique. Il prend en charge plusieurs langues, dont l'Arabe, et il est implémenté dans plusieurs langages de programmations. 
    %     \item \textbf{The MADAMIRA software:} développé au sein de l'université King Saud à l'Arabie Saoudite, ce Pos Tagger est d'une grande précision pour prédire correctement les étiquettes morpho-syntaxique des mots arabes.
    % \end{itemize}

\section{Aspect du langage}
    \subsection{Corpus}
    Un corpus est un ensemble vaste de texte structuré, uniformisé et spécialisé, généralement, dans un domaine précis.
    Les données stockés sont habituellement pré-traités soit manuellement par des experts, soit automatiquement à l'aide de programmes informatique. Un corpus peut contenir des textes dans une seule ou plusieurs langues.\\ 
    Afin de rendre les corpus plus utiles, ils sont souvent soumis à des processus de vérifications par des experts du domaine. Ces experts font passé les corpus, généralement, par des pré-traitement tel que l'étiquetage morpho-syntaxique, l'indication des lemmes des mots utilisés...\\
    Les corpus sont la base de connaissances principale en TALN et en linguistique. L'analyse et le traitement de divers types de corpus font également l'objet de nombreux travaux en reconnaissance vocale, traduction automatique, etc. où ils sont souvent utilisés pour créer des modèles d'apprentissage automatique et des modèles probabiliste tel que les modèles de Markov cachés.\\

    % Plusieurs corpus développés par des laboratoires de recherches sont disponibles gratuitement et en libre accès sur internet, on peut cité quelques un:

    % Wołk, K .; Marasek, K. (2015). "Tuned et GPU-accéléré l'extraction de données parallèle de corpus comparables". Notes de cours en intelligence artificielle . Springer: 32-40. ISBN  978-3-319-24032-9 .
    % Yoon, H., et Hirvela, A. (2004). Attitudes des élèves ESL envers l'utilisation du corpus dans l'écriture L2. Journal of Second Language Writing, 13(4), 257-283. Récupéré le 21 mars 2012.
    % \begin{itemize}
    %     \item \textbf{Gutenberg Corpus:} Une sélection de textes tirés des archives du projet Gutenberg, qui contient plus de 25000 livres électroniques gratuits (l'intégral des livres de William Shakespeare entre autre), tout le corpus est pré-traité, les textes sont segmentés en phrases et en mots, étiquetés et plein d'autres opérations très intéressantes.
    %     \item \textbf{Web and Chat Text:} Il est très important de considérer un langage moins formel que les livres de Shakespeare. La collection de \emph{Web and Chat Text} disponible sur la librairie open source \textbf{NLTK}\footnote{plate-forme leader pour le TALN en Python} est composés de discussions sur des forums de Firefox, des publicités personnelles et des critiques de films.
    %     \item \textbf{Brown Corpus:} Créé en 1961 par l'université de Brown il fut le premier corpus électronique anglais de millions de mots. Ce corpus contient des textes classés par genre (Religion, sport, fiction..). C'est une ressource pratique pour étudier les différences entre les catégories de texte.
    %     \item \textbf{Reuters Corpus:} Le Corpus Reuters contient 10.788 documents d'information totalisant 1,3 million de mots. Les documents ont été classés en 90 catégories. Cette répartition est destinée aux algorithmes d'apprentissage automatique supervisé qui prédisent la catégorie d'un document.\cite{nltk}
    % \end{itemize} 
    % Il existe plusieurs corpus développés par des laboratoires de recherches des universités mais qui ne sont pas accessible, ils sont généralement payant à des prix très élevés comme le \emph{Penn Treebank} développé par l'université de Pennsylvania qui en a fait tout un commerce.

    \subsection{WordNet}
    WordNet est une grande base de données lexicale. Les noms, les verbes, les adjectifs et les adverbes sont regroupés dans des ensembles de synonymes cognitifs (Synsets), chacun exprime un concept distinct.\\
    Les Synsets sont liés par des relations conceptuelles sémantiques et lexicales. WordNet est librement et publiquement disponible pour le téléchargement.
        \subsubsection{Structure}
        La principale relation entre les mots dans WordNet est la \emph{synonymie}, des mots qui dénotent le même concept et sont interchangeables dans de nombreux contextes, un synset contient une brève définition («gloss») et, dans la plupart des cas, une ou plusieurs phrases courtes illustrant l'utilisation.
        \begin{figure}[H]
            \centering
                \includegraphics[height=150pt,width=300pt]{img/chapter2/wordnet.jpg}
            \caption{Synset du WordNet Anglophone}
        \end{figure}

\section{Résumé automatique}
    \subsection{Définition}
    La procédure consiste à diminuer le contenu d'un document/texte avec un programme informatique. Le texte résultant (résumé) devrait contenir les idées les plus importantes du texte original.

    \emph{"La synthèse de texte est un processus qui consiste à distiller les informations les plus importantes d'une source pour produire une version abrégée"}\\
    — Page 1, «Advances in Automatic Text Summarization»\cite{aats}, 1999.

    Cette technique est de plus en plus nécessaires pour répondre à la croissance exponentielle des quantités  de données textuelles disponibles sur internet pour mieux reconnaître les informations pertinentes et pour les consommer plus rapidement.

    Dans un livre de 2014 intitulé «Automatic Text Summarization»\cite{atsjmtm}— Page 4-5, l'auteur à citer 6 raisons pour lesquelles nous avons besoin d'outils pour le résumé automatique:

    \begin{itemize}
        \item Les résumés réduisent le temps de lecture.
        \item Lors de la recherche de documents, les résumés facilitent le processus de sélection.
        \item L'utilisation de cette technique permet d'augmenter le nombre de textes traités.
        \item Le résumé automatique améliore l'efficacité de l'indexation.
        \item Les algorithmes de résumé automatique sont moins biaisés que les synthétiseurs humains.
        \item Les résumés personnalisés sont utiles dans les systèmes de Question/Réponse car ils fournissent des informations personnalisées.
    \end{itemize}

    \subsection{Domaines d'applications}    
    Il existe de nombreuses raisons et utilisations des résumés automatiques. Un exemple qui pourrait facilement venir à l'esprit est de créer un résumé d'un article de presse, mais il y a beaucoup plus de cas de résumés de texte que nous pouvons rencontrer tous les jours.

    Voici quelques exemples quotidiens de résumés de textes:
    \begin{itemize}
        \item Grandes lignes (prise de notes)
        \item Synopsis (scénario d'un film)
        \item Avis (Livre, article..)
        \item Biographie (Curriculum Vitae, nécrologie)
        \item Bulletins (prévisions météo)
        \item Histoires (chronologies des événements)
    \end{itemize}

    \subsection{Résumé extractif}
        Cette approche implique la sélection de phrases à partir d'un document source pour constituer un nouveau texte (résumé). Les techniques consistent à classer la pertinence des phrases afin de retenir celles qui sont les plus importantes pour garder le sens et la signification du texte original.

        La pertinence d'une phrase est calculée à base de plusieurs caractéristiques (Features), certaines on été citées dans un article publié dans \emph{"Journal of Computer Science"} en 2017 intitulé «A Review on Automatic Text Summarization Approaches»\cite{ratsa} :
        
        \begin{itemize}
            \item \textbf{Titre:} Les mots de titre apparaissant dans une phrase pourraient suggérer que la phrase contient des informations importantes
            \item \textbf{Position de la phrase:} Les phrases de début dans un document habituellement décrit les principales informations concernant document.
            \item \textbf{Longueur de la phrase:} Les phrases trop courtes peuvent contenir moins informations et les longues phrases ne sont pas appropriées à représenter le résumé.
            \item \textbf{Poids d'un mot/terme:} Mots ou termes qui ont une occurrence élevée dans un document est utilisé pour déterminer l'importance d'un phrase.
            \item \textbf{Nom propre/Entités nommées:} nom d'une personne, organisation un emplacement.. sont considéré comme porteur d'informations importantes.
        \end{itemize}   

        \subsubsection{État de l'art du Résumé extractif}
        \textbf{•La méthode basé TF-IDF\footnote{Term Frequency-Inverse Document Frequency, méthode de pondération souvent utilisée en recherche d'information.}:}
            Les phrases sont classées en fonction du terme et de la fréquence du document \cite{tfidf}[9]. La similarité entre le vecteur de requête et les vecteurs de phrases est calculé puis les phrases les plus importantes sont incluses dans le résumé. L'inconvénient est  la Redondance dans le résumé.\cite{surveysummarization}
            
        \textbf{•La méthode basé sur apprentissage automatique supervisé:} Les phrases sont classées sur la base de l'inclusion de phrases récapitulatives et de l'exclusion de phrases résumées. La classification est basée sur les caractéristiques de la pertinence des phrases dans le résumé. Les probabilités de classification sont faites sur la base de la technique statistique en utilisant la règle de Bayes. Dans ce cas, La redondance sera peut être réduite mais cette méthode reste complexe.\cite{surveysummarization}
            
        \textbf{•La méthode basé sur le Clustering\footnote{Le partitionnement de données, méthode d'analyse de donnée. Elle vise à diviser un ensemble de données en différents « paquets » homogènes, chaque sous-ensemble partagent des caractéristiques communes.}:} ces documents sont représentés en utilisant le TF-IDF de des dizaines de mots. La fréquence de terme utilisée dans ce contexte est le nombre moyen d'occurrences (par document) sur le cluster. La valeur IDF est calculée sur la base du corpus entier. Le synthétiseur prend des documents déjà groupés en entrée. Chaque cluster est considéré comme un thème.
            Le thème est représenté par des mots avec une fréquence de premier rang, dans le TF-IDF dans ce groupe.\\
            La sélection de la phrase est basée sur la similarité des phrases au thème du cluster. Le facteur suivant qui est pris en compte pour la sélection de la phrase est l'emplacement de la phrase dans le document (Li). Le dernier facteur qui augmente la score d'une phrase est sa similitude avec la première phrase du document auquel elle appartient
            (Fi).\\
            L'avantage du clustering est qu'il peut être utilisé pour grouper des phrases similaires dans plusieurs documents.\cite{surveysummarization}
            
        \textbf{•La méthode basé Logique Flou\footnote{Formalisée par Lotfi Zadeh en 1965, elle consiste à tenir compte de divers facteurs numériques pour aboutir à une décision qu'on souhaite acceptable.}:} Le système flou considère les caractéristiques des phrases telles que la longueur de la phrase, le mot du titre, mot-clé etc [10]. Score les phrases et extraire les phrases importantes sur la base de valeurs assignées. La fonction si-alors est utilisée pour trouver les phrases importantes.\\
        L'avantage de cette méthode est que les caractéristiques sont extraites et le score de fonctionnalité est donné comme entrée à \emph{Fuzzifier}, mais la performance reste limitée.\cite{surveysummarization}

    \subsection{Résumé abstractif}
        L'approche abstractive quant à elle, consiste en la génération de phrases entièrement nouvelles. Cette méthode est plus difficile, mais c'est aussi l'approche utilisée par l'humain. Elle est basée sur la sélection et la compression du contenu original.

        Récemment, les méthodes d'apprentissage en profondeur ont montré des résultats prometteurs pour le résumé automatique. Des approches ont été proposées inspirées par l'application de méthodes d'apprentissage en profondeur pour la traduction automatique.

        Principalement basées sur les Réseaux de neurones récurrents, ces dernières ont pu montrer des performances et des résultats assez impressionnantes, comme on peut le constater dans les deux articles «Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond»\cite{atsuss} et «A Neural Attention Model for Sentence Summarization»\cite{ruch} où plusieurs nouveau modèles basés sur l'apprentissage profond ont été proposés qui surpassent statistiquement tout les autres approches abstractives.

        \subsubsection{État de l'art du Résumé abstractif}
        \textbf{•Méthode basée sur un arbre:} Elle utilise un arbre de dépendance pour représenter le texte d'un document. Sur la base de relation entre les sous-arbres les phrases similaires sont sélectionné et le résumeur génère le résumé final [5].
            
        \textbf{•Méthode basée sur un modèle:} Elle utilise un modèle pour représenter le document. Les modèles linguistiques et les règles d'extraction sont utilisées pour identifier les extraits du texte sera cartographié dans les emplacements du modèle [5].

        \textbf{•Méthode basée sur l'ontologie:} Dans cette méthode, il est question d'utiliser le domaine d'ontologie ou base de connaissances au processus de résumé.
            Ramezani Majid, Feizi-Derakhshi
            Mohammad-Reza, 2015.
 
        

\section{Classification de texte}
    \subsection{Définition}
    La classification de textes a pour objectif de regrouper les textes similaires, c'est à dire thématiquement proches. L'intérêt d'une telle démarche est d'organiser les connaissances de façon à pouvoir effectuer, par la suite, une recherche ou une extraction d'information efficace. Elle est largement étudiée dans différentes communautés tels que le Data mining, les bases de données, l'apprentissage automatique et la recherche d'informations.

    \subsection{Domaines d'application}
    Plusieurs applications dans lesquels la classification de texte est couramment utilisée, on peut citer:

    \textbf{•Filtrage et organisation des informations d'actualités:} avec les quantités énormes de d'informations d'actualités en ligne, il est très difficile de catégoriser ces derniers manuellement, mais cela est possible grâce aux algorithmes de classifications. 

    \textbf{•Organisation des documents:} aider à la catégorisation de documents, un exemple d'application est les fichiers d'un ordinateur, distinguer les fichiers \emph{log}, \emph{système}, etc.

    \textbf{•Exploration des opinions:} souvent utilisé pour la catégorisation des commentaires sur internet.

    \textbf{•Classification des emails et filtrage des spams:} reconnaître l'objet de l'email ou déterminer les courriers indésirables de manière automatisée.

    \subsection{Technique de classification de texte}
    De nombreuse techniques ont été conçues pour la classification des textes. Puisque il est possible de modélisé ces derniers comme des données quantitatives généralement avec des fréquences de terme, la plupart des méthodes appliqués aux problèmes quantitatifs sont également applicables.\\    
    Quelques techniques on été citées dans un travail de recherche sur les algorithmes de classifications apparu dans le \emph{chapitre 6} du livre de 2012 «Mining Text Data»\cite{stca}:

    \begin{itemize}
        \item Arbres de décisions: la division hiérarchique de l'espace de données dans les arbres de décisions permet de créer des partitions de classe qui sont plus asymétriques en termes de distribution de leur classe.

        \item Basé règles: déterminer le modèles de mots le plus susceptibles d'être liés aux différentes classes, basé sur un ensemble de règles dans lesquelles le coté gauche correspond à un motif de mot, et le coté droit à une étiquette de classe.

        \item SVM: tentent de partitionner l'espace de données en utilisant des fonctions linéaires et non linéaires entre les classes.

        \item Réseau de neurones: avec l'utilisation des caractéristiques des mots, tout comme les SVMs les RNs essaient de trouver la limite la plus optimales entre les classes.

        \item Autres: presque tout les algorithmes de classifications peuvent être adaptés au données textuelles, tel que les algorithmes génétiques, etc.
    \end{itemize}


    \subsection{Catégorisation d'articles de presse}
    L’étude de la presse en ligne est devenue ces derniers temps un enjeu de recherche, et le nombre phénoménal de journaux, magazines et blogs sur internet qui ne cessent d'augmenter a fait de la La catégorisation d'articles de presse l'une des tâches les plus importantes de la classification de texte.

    Il s'agit de ranger des articles dans des classes, idéalement convenables, suivant une problématique donnée souvent le domaine traité par l'article dont Politique, Sport, Religion, Économie, etc.

    \subsubsection{État de l'art de la catégorisation d'articles de presse}
    NOT YET

\section{Système de traduction automatique}
La traduction est une activité en croissance très rapide de nos jours\cite{tradstat}. C'est un moyen actif de transférer la culture et la langue en communicant avec autrui \cite{tradcom}. Son aspect principal est de faire passer un message ou un texte rédigé dans une langue vers une autre langue en respectant le contexte et la grammaire.\\
Depuis très longtemps, la traduction automatique s'inscrit dans un ensemble de recherches appartenant aux domaine du traitement automatique du langage naturel.
    \subsection{Définition}
    La traduction automatique est le procédure par laquelle un ordinateur évalue un contenu source en entrée dans une langue donnée et délivre un contenu cible dans une langue différente en respectant les aspects lexicales, syntaxiques, sémantiques et morphologiques des deux langues.
    \subsection{Types de traduction automatique}
    Les traducteurs automatiques se distinguent dans les catégories suivantes en :
        \subsubsection{Traducteur automatique basé dictionnaire}
        Dans ce type de traducteur, la traduction automatique est basée sur les traductions des dictionnaires multilingue ordinaires, c'est a dire la traduction des mots du texte en entrée mot par mot en ne prenant pas en considération le sens des phrases. Il est a noter que les recherches dans le dictionnaire peuvent être faites avec ou sans l'utilisation des outils de langages dans l'analyse.
        \subsubsection{Traducteur automatique à base de règles de production}
        Dans ce type de traduction automatique, le traducteur prend en compte les Données étymologiques sur langues des document en entrée et le document cible fondamentalement récupéré de 
        lexiques et structures de phrases couvrant les règles sémantiques, morphologiques et syntaxiques principales de chaque langue individuellement. L'acquisition des phrases composant la langue du document source par le traducteur permet de produire phrases cible dans la langue du document cible sur le principe d'analyses morphologique, syntaxique et sémantique des deux langues.
        Les principaux travaux utilisant les bases de règles pour la traduction ont été créés au milieu des années 1970.\cite{surveyTraduction}
        les Traducteurs automatiques a base de règles de production sont capables de produire des traductions avec une qualité raisonnable, mais la construction du système prend beaucoup de temps et de main-d'œuvre parce que ces ressources linguistiques doivent être fabriquées à la main.De plus, il est très difficile de corriger l'entrée ou d'ajouter de nouvelles règles au système pour générer une traduction.\cite{jean}
        \subsubsection{Traducteur automatique à base d'exemples}
        L'approche à base d'exemples repose sur un ensemble de phrases traduites préalablement. Lors de la traduction le processus vérifie si la phrase a l'entrée est parmi les exemples, dans le cas positif il fournit directement sa traduction sinon il s'inspire d'autres phrases contenant un ou plusieurs ensembles de mots similaires et fournit la traduction de ces mots selon les exemples dont il dispose.\cite{setif}
        \subsubsection{Traducteur automatique basé connaissances}
        Ce type de traducteur automatique se concentre sur l'aspect lexicale que représente un domaine défini (propre a un seul domaine) exemple: traducteur pour les articles de recherches, Médecine, etc.\cite{surveyTraduction}
        \subsubsection{Traducteur automatique basé sur un corpus}
        La traduction automatique basée sur le corpus (CBMT) est générée sur l'analyse des corpus de texte bilingues. Cette technique est utilisée depuis 1989 qui se base sur un corpus de traduction pour la machine. Cette technique a régné sur les autres compte tenu de son exactitude, notamment a l'ajout d'exemples au système qui peut améliorer les performances de ce dernier puisqu'il est basé sur les données, bien que l'accumulation et la gestion de l'énorme corpus de données bilingues puissent également être coûteuses.\cite{jean}
        \subsubsection{Traducteur automatique basé contexte}
        La traduction automatique basé contexte est un nouveau paradigme pour la traduction automatique basé corpus qui nécessite : un vaste corpus de texte cible monolingue et un dictionnaire bilingue complet.CONTRAST [8] et REFTEX [9] sont des exemples de la traduction automatique basé contexte.


\section{Conclusion}
Dans ce chapitre, nous avons essayer de faire le tour sur les techniques du Traitement Automatique du Langage Naturel. Nous avons d'abord présenté cette science, son importances et ses domaines d'application. Ensuite, nous avons examiner l'état de l'art de chaque technique en expliquant les processus et les méthodes. Et enfin, nous avons passer rapidement certaines applications actuelles du TALN.
Le prochain point aborder quant à lui, présentera notre contribution, sa conception générale et ses différentes fonctionnalités.  